# ğŸ§ª A/B Testing in Databricks â€” EDA + Statistical Significance

## ğŸ“– What is A/B Testing?
A/B testing is an experimental method to compare two versions of something â€” **Variant A** (control) and **Variant B** (treatment) â€” to see which performs better for a specific metric.

In this case, weâ€™re testing **user engagement and monetization metrics** for two variants.

---

## ğŸ“Š Metrics in Our Test

- **Impressions**: Number of times an item/ad was shown  
- **Clicks**: Number of times it was clicked  
- **Orders**: Purchases made after clicking  
- **Revenue**: Money earned from orders  

From these, we calculate:

- **Click-Through Rate (CTR)** = `Clicks / Impressions Ã— 100`
- **Conversion Rate** = `Orders / Clicks Ã— 100`
- **Average Revenue per User (ARPU)** = `Revenue / Number of Users`

---

## ğŸ“¦ Synthetic Experiment Scenario

**Experiment**: Testing a new ad ranking model on the Instacart homepage.  

- **Variant A (Control)** â†’ current ranking algorithm  
- **Variant B (Treatment)** â†’ new ranking algorithm  

**Goal**: Increase CTR and conversion rate, while maintaining or improving revenue.

---

## ğŸ“ Dataset Columns

| Column       | Type    | Description                                       |
|--------------|---------|---------------------------------------------------|
| user_id      | int     | Unique shopper ID                                 |
| variant      | string  | 'A' (control) or 'B' (treatment)                   |
| impressions  | int     | Number of ads shown                               |
| clicks       | int     | Number of ads clicked                             |
| orders       | int     | Orders placed after clicking                      |
| revenue      | float   | Total revenue from orders                         |

---

## ğŸ”¬ Why Statistical Testing?
Raw numbers can be misleading â€” differences might be due to **chance**.  

We use **p-values** to test statistical significance:
- **p < 0.05** â†’ Significant (unlikely to be random)
- **p â‰¥ 0.05** â†’ Not significant (could be random noise)

**Tests Used:**
- CTR% â†’ Chi-square test  
- Conversion% â†’ Chi-square test  
- Revenue/User â†’ t-test  

---

## ğŸš€ Project Highlights
- Data cleaning to remove invalid metrics (e.g., clicks > impressions)
- Metric calculation in Databricks SQL
- Statistical significance testing in Python (Chi-square, t-test)
- Visualization using Matplotlib + Seaborn
- Clear documentation of A/B testing methodology

---

## ğŸ“Š Results Summary

| Metric            | Variant A | Variant B | p-value | Significant? |
|-------------------|-----------|-----------|---------|--------------|
| CTR%              | 8.15%     | 9.67%     | 0.0000  | âœ… Yes       |
| Conversion%       | 11.81%    | 12.71%    | 0.1565  | âŒ No        |
| Avg Revenue/User  | $218.49   | $280.69   | 0.0000  | âœ… Yes       |

---

## ğŸ“ˆ Visual Results

### CTR Comparison
![CTR Comparison](images/ctr_comparision.png)

### Revenue per User Comparison
![Conversion Rate Comparison](images/ctr_comparision.png)

### Conversion Rate Comparison
![Revenue Comparison](images/conversion_rate_comparision.png)

---

## âœ… Conclusion
Variant B has a **significantly higher CTR and ARPU**.  
Conversion rate difference is **not statistically significant**.  

**Recommendation:** Roll out Variant B and monitor long-term performance.

---

## ğŸ›  Tech Stack
- **Databricks SQL** â†’ Data preparation & aggregation
- **Python (Pandas, Seaborn, SciPy)** â†’ Statistical testing & visualization
- **Matplotlib** â†’ Charts

---

## ğŸ“‚ Repo Structure
```
ab-testing-databricks/
â”œâ”€â”€ notebook/
â”‚ â””â”€â”€ ab_testing_databricks.ipynb
â”œâ”€â”€ images/
â”‚ â”œâ”€â”€ ctr_comparison.png
â”‚ â”œâ”€â”€ revenue_comparison.png
â”‚ â””â”€â”€ conversion_rate_comparison.png
â”œâ”€â”€ data/ # optional
â”‚ â””â”€â”€ ab_test_sample.csv
â””â”€â”€ README.md
```
---

## ğŸ“„ Author
**Rehan Chaudhry**  
[GitHub Portfolio](https://github.com/rehansc)  
[LinkedIn](https://www.linkedin.com/in/rehanchaudhry/)
